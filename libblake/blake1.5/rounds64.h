//ROUND :::::::::::: 0
//round : 0 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[0][0]], c[sigma[0][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[0][0+1]], c[sigma[0][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 0 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[0][2]], c[sigma[0][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[0][2+1]], c[sigma[0][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 0 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[0][4]], c[sigma[0][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[0][4+1]], c[sigma[0][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 0 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[0][6]], c[sigma[0][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[0][6+1]], c[sigma[0][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 0 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[0][8]], c[sigma[0][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[0][8+1]], c[sigma[0][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 0 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[0][10]], c[sigma[0][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[0][10+1]], c[sigma[0][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 0 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[0][12]], c[sigma[0][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[0][12+1]], c[sigma[0][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 0 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[0][14]], c[sigma[0][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[0][14+1]], c[sigma[0][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 1
//round : 1 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[1][0]], c[sigma[1][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[1][0+1]], c[sigma[1][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 1 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[1][2]], c[sigma[1][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[1][2+1]], c[sigma[1][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 1 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[1][4]], c[sigma[1][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[1][4+1]], c[sigma[1][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 1 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[1][6]], c[sigma[1][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[1][6+1]], c[sigma[1][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 1 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[1][8]], c[sigma[1][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[1][8+1]], c[sigma[1][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 1 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[1][10]], c[sigma[1][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[1][10+1]], c[sigma[1][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 1 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[1][12]], c[sigma[1][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[1][12+1]], c[sigma[1][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 1 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[1][14]], c[sigma[1][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[1][14+1]], c[sigma[1][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 2
//round : 2 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[2][0]], c[sigma[2][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[2][0+1]], c[sigma[2][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 2 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[2][2]], c[sigma[2][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[2][2+1]], c[sigma[2][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 2 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[2][4]], c[sigma[2][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[2][4+1]], c[sigma[2][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 2 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[2][6]], c[sigma[2][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[2][6+1]], c[sigma[2][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 2 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[2][8]], c[sigma[2][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[2][8+1]], c[sigma[2][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 2 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[2][10]], c[sigma[2][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[2][10+1]], c[sigma[2][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 2 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[2][12]], c[sigma[2][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[2][12+1]], c[sigma[2][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 2 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[2][14]], c[sigma[2][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[2][14+1]], c[sigma[2][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 3
//round : 3 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[3][0]], c[sigma[3][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[3][0+1]], c[sigma[3][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 3 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[3][2]], c[sigma[3][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[3][2+1]], c[sigma[3][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 3 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[3][4]], c[sigma[3][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[3][4+1]], c[sigma[3][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 3 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[3][6]], c[sigma[3][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[3][6+1]], c[sigma[3][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 3 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[3][8]], c[sigma[3][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[3][8+1]], c[sigma[3][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 3 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[3][10]], c[sigma[3][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[3][10+1]], c[sigma[3][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 3 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[3][12]], c[sigma[3][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[3][12+1]], c[sigma[3][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 3 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[3][14]], c[sigma[3][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[3][14+1]], c[sigma[3][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 4
//round : 4 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[4][0]], c[sigma[4][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[4][0+1]], c[sigma[4][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 4 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[4][2]], c[sigma[4][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[4][2+1]], c[sigma[4][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 4 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[4][4]], c[sigma[4][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[4][4+1]], c[sigma[4][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 4 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[4][6]], c[sigma[4][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[4][6+1]], c[sigma[4][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 4 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[4][8]], c[sigma[4][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[4][8+1]], c[sigma[4][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 4 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[4][10]], c[sigma[4][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[4][10+1]], c[sigma[4][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 4 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[4][12]], c[sigma[4][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[4][12+1]], c[sigma[4][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 4 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[4][14]], c[sigma[4][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[4][14+1]], c[sigma[4][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 5
//round : 5 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[5][0]], c[sigma[5][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[5][0+1]], c[sigma[5][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 5 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[5][2]], c[sigma[5][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[5][2+1]], c[sigma[5][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 5 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[5][4]], c[sigma[5][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[5][4+1]], c[sigma[5][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 5 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[5][6]], c[sigma[5][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[5][6+1]], c[sigma[5][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 5 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[5][8]], c[sigma[5][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[5][8+1]], c[sigma[5][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 5 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[5][10]], c[sigma[5][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[5][10+1]], c[sigma[5][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 5 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[5][12]], c[sigma[5][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[5][12+1]], c[sigma[5][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 5 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[5][14]], c[sigma[5][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[5][14+1]], c[sigma[5][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 6
//round : 6 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[6][0]], c[sigma[6][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[6][0+1]], c[sigma[6][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 6 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[6][2]], c[sigma[6][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[6][2+1]], c[sigma[6][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 6 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[6][4]], c[sigma[6][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[6][4+1]], c[sigma[6][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 6 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[6][6]], c[sigma[6][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[6][6+1]], c[sigma[6][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 6 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[6][8]], c[sigma[6][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[6][8+1]], c[sigma[6][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 6 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[6][10]], c[sigma[6][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[6][10+1]], c[sigma[6][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 6 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[6][12]], c[sigma[6][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[6][12+1]], c[sigma[6][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 6 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[6][14]], c[sigma[6][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[6][14+1]], c[sigma[6][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 7
//round : 7 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[7][0]], c[sigma[7][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[7][0+1]], c[sigma[7][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 7 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[7][2]], c[sigma[7][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[7][2+1]], c[sigma[7][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 7 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[7][4]], c[sigma[7][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[7][4+1]], c[sigma[7][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 7 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[7][6]], c[sigma[7][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[7][6+1]], c[sigma[7][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 7 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[7][8]], c[sigma[7][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[7][8+1]], c[sigma[7][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 7 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[7][10]], c[sigma[7][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[7][10+1]], c[sigma[7][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 7 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[7][12]], c[sigma[7][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[7][12+1]], c[sigma[7][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 7 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[7][14]], c[sigma[7][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[7][14+1]], c[sigma[7][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 8
//round : 8 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[8][0]], c[sigma[8][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[8][0+1]], c[sigma[8][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 8 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[8][2]], c[sigma[8][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[8][2+1]], c[sigma[8][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 8 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[8][4]], c[sigma[8][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[8][4+1]], c[sigma[8][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 8 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[8][6]], c[sigma[8][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[8][6+1]], c[sigma[8][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 8 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[8][8]], c[sigma[8][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[8][8+1]], c[sigma[8][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 8 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[8][10]], c[sigma[8][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[8][10+1]], c[sigma[8][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 8 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[8][12]], c[sigma[8][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[8][12+1]], c[sigma[8][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 8 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[8][14]], c[sigma[8][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[8][14+1]], c[sigma[8][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 9
//round : 9 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[9][0]], c[sigma[9][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[9][0+1]], c[sigma[9][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 9 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[9][2]], c[sigma[9][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[9][2+1]], c[sigma[9][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 9 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[9][4]], c[sigma[9][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[9][4+1]], c[sigma[9][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 9 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[9][6]], c[sigma[9][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[9][6+1]], c[sigma[9][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 9 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[9][8]], c[sigma[9][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[9][8+1]], c[sigma[9][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 9 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[9][10]], c[sigma[9][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[9][10+1]], c[sigma[9][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 9 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[9][12]], c[sigma[9][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[9][12+1]], c[sigma[9][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 9 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[9][14]], c[sigma[9][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[9][14+1]], c[sigma[9][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 10
//round : 10 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[10][0]], c[sigma[10][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[10][0+1]], c[sigma[10][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 10 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[10][2]], c[sigma[10][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[10][2+1]], c[sigma[10][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 10 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[10][4]], c[sigma[10][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[10][4+1]], c[sigma[10][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 10 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[10][6]], c[sigma[10][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[10][6+1]], c[sigma[10][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 10 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[10][8]], c[sigma[10][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[10][8+1]], c[sigma[10][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 10 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[10][10]], c[sigma[10][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[10][10+1]], c[sigma[10][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 10 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[10][12]], c[sigma[10][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[10][12+1]], c[sigma[10][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 10 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[10][14]], c[sigma[10][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[10][14+1]], c[sigma[10][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 11
//round : 11 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[11][0]], c[sigma[11][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[11][0+1]], c[sigma[11][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 11 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[11][2]], c[sigma[11][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[11][2+1]], c[sigma[11][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 11 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[11][4]], c[sigma[11][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[11][4+1]], c[sigma[11][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 11 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[11][6]], c[sigma[11][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[11][6+1]], c[sigma[11][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 11 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[11][8]], c[sigma[11][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[11][8+1]], c[sigma[11][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 11 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[11][10]], c[sigma[11][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[11][10+1]], c[sigma[11][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 11 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[11][12]], c[sigma[11][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[11][12+1]], c[sigma[11][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 11 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[11][14]], c[sigma[11][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[11][14+1]], c[sigma[11][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 12
//round : 12 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[12][0]], c[sigma[12][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[12][0+1]], c[sigma[12][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 12 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[12][2]], c[sigma[12][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[12][2+1]], c[sigma[12][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 12 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[12][4]], c[sigma[12][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[12][4+1]], c[sigma[12][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 12 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[12][6]], c[sigma[12][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[12][6+1]], c[sigma[12][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 12 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[12][8]], c[sigma[12][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[12][8+1]], c[sigma[12][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 12 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[12][10]], c[sigma[12][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[12][10+1]], c[sigma[12][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 12 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[12][12]], c[sigma[12][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[12][12+1]], c[sigma[12][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 12 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[12][14]], c[sigma[12][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[12][14+1]], c[sigma[12][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 13
//round : 13 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[13][0]], c[sigma[13][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[13][0+1]], c[sigma[13][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 13 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[13][2]], c[sigma[13][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[13][2+1]], c[sigma[13][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 13 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[13][4]], c[sigma[13][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[13][4+1]], c[sigma[13][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 13 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[13][6]], c[sigma[13][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[13][6+1]], c[sigma[13][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 13 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[13][8]], c[sigma[13][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[13][8+1]], c[sigma[13][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 13 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[13][10]], c[sigma[13][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[13][10+1]], c[sigma[13][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 13 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[13][12]], c[sigma[13][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[13][12+1]], c[sigma[13][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 13 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[13][14]], c[sigma[13][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[13][14+1]], c[sigma[13][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 14
//round : 14 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[14][0]], c[sigma[14][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[14][0+1]], c[sigma[14][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 14 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[14][2]], c[sigma[14][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[14][2+1]], c[sigma[14][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 14 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[14][4]], c[sigma[14][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[14][4+1]], c[sigma[14][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 14 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[14][6]], c[sigma[14][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[14][6+1]], c[sigma[14][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 14 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[14][8]], c[sigma[14][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[14][8+1]], c[sigma[14][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 14 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[14][10]], c[sigma[14][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[14][10+1]], c[sigma[14][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 14 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[14][12]], c[sigma[14][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[14][12+1]], c[sigma[14][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 14 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[14][14]], c[sigma[14][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[14][14+1]], c[sigma[14][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


//ROUND :::::::::::: 15
//round : 15 , i: 0
 (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[15][0]], c[sigma[15][0+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v4) ))+XOR64(m[sigma[15][0+1]], c[sigma[15][0]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v0) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v12) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v8) )),11); 


//round : 15 , i: 2
 (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[15][2]], c[sigma[15][2+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v5) ))+XOR64(m[sigma[15][2+1]], c[sigma[15][2]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v1) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v13) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v9) )),11); 


//round : 15 , i: 4
 (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[15][4]], c[sigma[15][4+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v6) ))+XOR64(m[sigma[15][4+1]], c[sigma[15][4]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v2) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v14) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v10) )),11); 


//round : 15 , i: 6
 (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[15][6]], c[sigma[15][6+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v7) ))+XOR64(m[sigma[15][6+1]], c[sigma[15][6]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v3) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v15) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v11) )),11); 


//round : 15 , i: 8
 (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[15][8]], c[sigma[15][8+1]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),32); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),25); 
     (*v0)  = ADD64(( (*v0) ),( (*v5) ))+XOR64(m[sigma[15][8+1]], c[sigma[15][8]]); 
     (*v15)  = ROT64(XOR64(( (*v15) ),( (*v0) )),16); 
     (*v10)  = ADD64(( (*v10) ),( (*v15) )); 
     (*v5)  = ROT64(XOR64(( (*v5) ),( (*v10) )),11); 


//round : 15 , i: 10
 (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[15][10]], c[sigma[15][10+1]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),32); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),25); 
     (*v1)  = ADD64(( (*v1) ),( (*v6) ))+XOR64(m[sigma[15][10+1]], c[sigma[15][10]]); 
     (*v12)  = ROT64(XOR64(( (*v12) ),( (*v1) )),16); 
     (*v11)  = ADD64(( (*v11) ),( (*v12) )); 
     (*v6)  = ROT64(XOR64(( (*v6) ),( (*v11) )),11); 


//round : 15 , i: 12
 (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[15][12]], c[sigma[15][12+1]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),32); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),25); 
     (*v2)  = ADD64(( (*v2) ),( (*v7) ))+XOR64(m[sigma[15][12+1]], c[sigma[15][12]]); 
     (*v13)  = ROT64(XOR64(( (*v13) ),( (*v2) )),16); 
     (*v8)  = ADD64(( (*v8) ),( (*v13) )); 
     (*v7)  = ROT64(XOR64(( (*v7) ),( (*v8) )),11); 


//round : 15 , i: 14
 (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[15][14]], c[sigma[15][14+1]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),32); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),25); 
     (*v3)  = ADD64(( (*v3) ),( (*v4) ))+XOR64(m[sigma[15][14+1]], c[sigma[15][14]]); 
     (*v14)  = ROT64(XOR64(( (*v14) ),( (*v3) )),16); 
     (*v9)  = ADD64(( (*v9) ),( (*v14) )); 
     (*v4)  = ROT64(XOR64(( (*v4) ),( (*v9) )),11); 


